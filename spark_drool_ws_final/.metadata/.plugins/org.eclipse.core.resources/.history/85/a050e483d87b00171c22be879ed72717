package dynamic;

import java.io.FileReader;
import java.util.HashMap;
import java.util.Hashtable;
import java.util.List;

import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SQLContext;
import org.apache.spark.sql.SparkSession;
import org.json.simple.JSONArray;
import org.json.simple.JSONObject;
import org.json.simple.parser.JSONParser;

public class SparkRuleEngine 
{
	public static void main(String args[])
	{
		SparkSession spark = SparkSession.builder().master("local")
				.appName("Rule Engine")
				.config("spark.some.config.option", "some-value").getOrCreate();
		
		JSONParser parser = new JSONParser();
		JSONArray jArray = (JSONArray) parser
				.parse(new FileReader(
						"/home/neil/.node-red/flows_neil-inspiron-3521.json"));

		for (Object obj : jArray) 
		{
			JSONObject jsonObject = (JSONObject) obj;
			System.out.println(jsonObject);

			String topic = (String) jsonObject.get("topic");
			System.out.println(topic);

			int payload = (Integer) jsonObject.get("payload");
			System.out.println(payload);
		}
/*		
		
		
		
		
		
		
		Dataset<Row> df1 = spark.read().json("/home/neil/.node-red/flows_neil-inspiron-3521.json");
		
		df1.show();
		df1.printSchema();
		df1.select("LOG_ID").show();
		
		df1.createOrReplaceTempView("data1");
	
		Dataset<Row> temp3 = spark.sql("SELECT * from data1 where LOG_ID="+i);
		
		DroolTest dt = new DroolTest();
		List<HashMap> update = dt.droolProcess(temp3, temp4, tempPosition.get(i), presPos.get(i), gasPos.get(i));
		
		updateJSON(spark, i, df1, df2, update);
	}

	private static void updateJSON(SparkSession spark, int i, Dataset<Row> df1,
			Dataset<Row> df2, List<HashMap> updates) 
	{
		if(!updates.isEmpty())
		{
			HashMap<String, Integer> h1 = (HashMap<String, Integer>)updates.get(0);
			HashMap<String, Double> h2 = (HashMap<String, Double>) updates.get(1);
			
			System.out.println("temperature 1 updated : "+ h1.get("TEMP1"));
			System.out.println("temperature 2 updated : "+ h1.get("TEMP2"));
			
			System.out.println("pressure 1 updated : "+ h1.get("PRE1"));
			System.out.println("pressure 2 updated : "+ h1.get("PRE2"));
			
			System.out.println("gas 1 updated : "+ h2.get("GAS1"));
			System.out.println("gas 2 updated : "+ h2.get("GAS2"));
			
			df1.registerTempTable("citytemps1");
			df2.registerTempTable("citytemps2");
			
			allUDFs(spark, h1, h2);
			
			SQLContext sq7 = new SQLContext(spark);
			Dataset<Row> b1 = sq7.sql("SELECT LOG_ID, CHANGE_PRE1(0) AS PRESSURE, GAS, CHANGE_TEMP1(0) AS TEMPERATURE FROM citytemps1 where LOG_ID="+i);
			Dataset<Row> b2 = sq7.sql("SELECT LOG_ID, CHANGE_PRE2(0) AS PRESSURE, GAS, CHANGE_TEMP2(0) AS TEMPERATURE FROM citytemps2 where LOG_ID="+(i+1));
			System.out.println(b1.head());
			System.out.println(b2.head());

			b1.write().json("/home/neil/Neil_Work/MS_SJSU/scala_spark_learning/Spark_Scala/git_ws/SparkDrools/loglog"+i);
			b2.write().json("/home/neil/Neil_Work/MS_SJSU/scala_spark_learning/Spark_Scala/git_ws/SparkDrools/loglog"+(i+1));
		}
	}

	private static void allUDFs(SparkSession spark,
			HashMap<String, Integer> h1, HashMap<String, Double> h2) 
	{
		// FUNCTIONS
		
	}*/
}
