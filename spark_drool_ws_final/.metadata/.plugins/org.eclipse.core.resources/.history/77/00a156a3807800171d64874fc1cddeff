package sample.engine;

import java.util.HashMap;
import java.util.Hashtable;
import java.util.List;

import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SQLContext;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.api.java.UDF1;
import org.apache.spark.sql.types.DataTypes;

public class SparkRuleEngine
{
	public static void main(String args[])
	{
		SparkSession spark = SparkSession.builder().master("local")
				.appName("Rule Engine")
				.config("spark.some.config.option", "some-value").getOrCreate();
		
		for(int i=1;i<=2;i+=2)
		{
			Dataset<Row> df1 = spark.read().json("/home/neil/Neil_Work/MS_SJSU/scala_spark_learning/Spark_Scala/logs_input/log"+i+".json");
			Dataset<Row> df2 = spark.read().json("/home/neil/Neil_Work/MS_SJSU/scala_spark_learning/Spark_Scala/logs_input/log"+(i+1)+".json");

			df1.show();
			df1.printSchema();
			df1.select("LOG_ID").show();
			
			df2.show();
			df2.printSchema();
			df2.select("LOG_ID").show();
			
			Hashtable<Integer, Integer> tempPosition = new Hashtable<Integer, Integer>();
			Hashtable<Integer, Integer> presPos = new Hashtable<Integer, Integer>();
			Hashtable<Integer, Integer> gasPos = new Hashtable<Integer, Integer>();
			for(int j=1;j<=6;j++)
			{
				tempPosition.put(j, 4);
				presPos.put(j, 3);
				gasPos.put(j, 1);
			}
			for(int j=7;j<=12;j++)
			{
				tempPosition.put(j, 2);
				presPos.put(j, 3);
				gasPos.put(j, 4);
			}
			df1.createOrReplaceTempView("data1");
			df2.createOrReplaceTempView("data2");
		
			Dataset<Row> temp3 = spark.sql("SELECT * from data1 where LOG_ID="+i);
			Dataset<Row> temp4 = spark.sql("SELECT * from data2 where LOG_ID="+(i+1));
			
			DroolTest dt = new DroolTest();
			List<HashMap> update = dt.droolProcess(temp3, temp4, tempPosition.get(i), presPos.get(i), gasPos.get(i));
			updateJSON(spark, i, df1, df2, update);
		}
	}

	private static void updateJSON(SparkSession spark, int i, Dataset<Row> df1,
			Dataset<Row> df2, List<HashMap> updates) 
	{
		if(!updates.isEmpty())
		{
			HashMap<String, Integer> h1 = (HashMap<String, Integer>)updates.get(0);
			HashMap<String, Double> h2 = (HashMap<String, Double>) updates.get(1);
			
			System.out.println("temperature 1 updated : "+ h1.get("TEMP1"));
			System.out.println("temperature 2 updated : "+ h1.get("TEMP2"));
			
			System.out.println("pressure 1 updated : "+ h1.get("PRE1"));
			System.out.println("pressure 2 updated : "+ h1.get("PRE2"));
			
			System.out.println("gas 1 updated : "+ h2.get("GAS1"));
			System.out.println("gas 2 updated : "+ h2.get("GAS2"));
			
			df1.registerTempTable("citytemps1");
			df2.registerTempTable("citytemps2");
			
			SQLContext sq1 = new SQLContext(spark);
			sq1.udf().register("CHANGE_TEMP1", new UDF1<Integer, Integer>() {
				@Override
				public Integer call(Integer a) {
					return h1.get("TEMP1");
				}
			}, DataTypes.IntegerType);
			
			SQLContext sq2 = new SQLContext(spark);
			sq2.udf().register("CHANGE_TEMP2", new UDF1<Integer, Integer>() {
				@Override
				public Integer call(Integer a) {
					return h1.get("TEMP2");
				}
			}, DataTypes.IntegerType);
			
			SQLContext sq3 = new SQLContext(spark);
			sq3.udf().register("CHANGE_GAS1", new UDF1<Double, Double>() {
				@Override
				public Double call(Double a) {
					return h2.get("GAS1");
				}
			}, DataTypes.DoubleType);
			
			SQLContext sq4 = new SQLContext(spark);
			sq4.udf().register("CHANGE_GAS2", new UDF1<Double, Double>() {
				@Override
				public Double call(Double a) {
					return h2.get("GAS2");
				}
			}, DataTypes.DoubleType);
			
			SQLContext sq5 = new SQLContext(spark);
			sq5.udf().register("CHANGE_PRE1", new UDF1<Integer, Integer>() {
				@Override
				public Integer call(Integer a) {
					return h1.get("PRE1");
				}
			}, DataTypes.IntegerType);
			
			SQLContext sq6 = new SQLContext(spark);
			sq6.udf().register("CHANGE_PRE2", new UDF1<Integer, Integer>() {
				@Override
				public Integer call(Integer a) {
					return h1.get("PRE2");
				}
			}, DataTypes.IntegerType);
			
			Dataset<Row> b1 = sq1.sql("SELECT LOG_ID,  CHANGE_PRE1(0) AS PRESSURE, GAS, CHANGE_TEMP1(0) AS TEMPERATURE FROM citytemps1 where LOG_ID="+i);
			Dataset<Row> b2 = sq2.sql("SELECT LOG_ID, CHANGE_PRE2(0) AS PRESSURE, GAS, CHANGE_TEMP2(0) AS TEMPERATURE FROM citytemps2 where LOG_ID="+(i+1));
			System.out.println(b1.head());
			System.out.println(b2.head());
		
			b1.write().json("/home/neil/Neil_Work/MS_SJSU/scala_spark_learning/Spark_Scala/loglog"+i);
			b2.write().json("/home/neil/Neil_Work/MS_SJSU/scala_spark_learning/Spark_Scala/loglog"+(i+1));
		}
	}
}
